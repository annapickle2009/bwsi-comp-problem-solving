{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Supervised Learning: More Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#Please read the sklearn documentation if you need to figure out the inputs and outputs of these functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PART 1: SIMPLE LINEAR REGRESSION\n",
    "\n",
    "\n",
    "\n",
    " We'll analyze the relationship between study hours and exam scores using three different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: Study Hours vs Exam Scores\n",
    "data = {\n",
    "    'Hours_Studied': [1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5, 6.0],\n",
    "    'Exam_Score': [45, 51, 58, 62, 68, 73, 78, 84, 88, 93]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Dataset:\")\n",
    "print(df)\n",
    "\n",
    "# Visualize the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['Hours_Studied'], df['Exam_Score'], color='blue', s=100)\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.title('Study Hours vs Exam Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TASK 1: Manual Least Squares Calculation\n",
    "\n",
    "\n",
    "\n",
    " Calculate the slope (m) and intercept (b) for y = mx + b using the **FIRST 5 DATA POINTS ONLY**.\n",
    "\n",
    "\n",
    "\n",
    " **Formulas:**\n",
    "\n",
    " - Slope: $m = \\frac{n\\sum xy - \\sum x \\sum y}{n\\sum x^2 - (\\sum x)^2}$\n",
    "\n",
    " - Intercept: $b = \\frac{\\sum y - m\\sum x}{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use first 5 data points\n",
    "x_manual = df['Hours_Studied'][:5].values\n",
    "y_manual = df['Exam_Score'][:5].values\n",
    "\n",
    "n = len(x_manual)\n",
    "\n",
    "# TODO: Calculate sums needed\n",
    "sum_x = np.sum(x_manual)\n",
    "sum_y = np.sum(y_manual)\n",
    "sum_xy = np.sum(x_manual * y_manual)\n",
    "sum_x_squared = np.sum(x_manual ** 2)\n",
    "\n",
    "# TODO: Calculate slope and intercept using formulas above\n",
    "m_manual = (n * sum_xy - sum_x * sum_y) / (n * sum_x_squared - (sum_x ** 2))\n",
    "b_manual = (sum_y - m_manual * sum_x) / n\n",
    "\n",
    "print(\"TASK 1 - Manual Calculation (first 5 points):\")\n",
    "print(f\"Slope (m): {m_manual}\")\n",
    "print(f\"Intercept (b): {b_manual}\")\n",
    "print(f\"Equation: y = {m_manual:.2f}x + {b_manual:.2f}\\n\")\n",
    "\n",
    "# TODO: Make predictions and calculate errors\n",
    "y_pred_manual = m_manual * x_manual + b_manual\n",
    "mae_manual = mean_absolute_error(y_manual, y_pred_manual)\n",
    "mse_manual = mean_squared_error(y_manual, y_pred_manual)\n",
    "\n",
    "print(f\"MAE: {mae_manual:.2f}\")\n",
    "print(f\"MSE: {mse_manual:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TASK 2: NumPy lstsq (Full Dataset)\n",
    "\n",
    "\n",
    "\n",
    " Use `np.linalg.lstsq` to find the best fit line for all 10 data points.\n",
    "\n",
    "\n",
    "\n",
    " #### Why Add a Column of Ones?\n",
    "\n",
    "\n",
    "\n",
    " When we use `np.linalg.lstsq`, we're solving the matrix equation: **Xβ = y**\n",
    "\n",
    "\n",
    "\n",
    " For a linear regression equation **y = mx + b**, we need to find both:\n",
    "\n",
    " - **m** (slope)\n",
    "\n",
    " - **b** (intercept)\n",
    "\n",
    "\n",
    "\n",
    " The design matrix must be structured so that matrix multiplication gives us: **b·1 + m·x**\n",
    "\n",
    "\n",
    "\n",
    " ```\n",
    "\n",
    " Design Matrix (X):        Coefficients (β):     Result (y):\n",
    "\n",
    " [1  x₁]                   [b]                   [b·1 + m·x₁]\n",
    "\n",
    " [1  x₂]           ×       [m]          =        [b·1 + m·x₂]\n",
    "\n",
    " [1  x₃]                                         [b·1 + m·x₃]\n",
    "\n",
    " ...\n",
    "\n",
    " ```\n",
    "\n",
    "\n",
    "\n",
    " **Key Points:**\n",
    "\n",
    " - The column of ones multiplies with the intercept **b** to add that constant term to each prediction\n",
    "\n",
    " - The column of x values multiplies with the slope **m** to add the variable component\n",
    "\n",
    " - **Without the ones column:** We'd only fit lines through the origin (b=0): y = mx\n",
    "\n",
    " - **With the ones column:** We can fit any line: y = mx + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prepare the design matrix\n",
    "# Hint: Use np.column_stack to combine a column of ones with the Hours_Studied values\n",
    "X_numpy = np.column_stacck([np.ones(len(df)), df['Hours Studied'].values])\n",
    "y_numpy = df['Exam_Score'].values\n",
    "\n",
    "# TODO: Use np.linalg.lstsq\n",
    "solution, residuals, rank, s = np.linalg.lstsq(X_numpy, y_numpy, rcond=None)\n",
    "\n",
    "# TODO: Extract coefficients\n",
    "b_numpy = solution[0]\n",
    "m_numpy = solution[1]\n",
    "\n",
    "print(\"TASK 2 - NumPy lstsq (all 10 points):\")\n",
    "print(f\"Slope (m): {m_numpy}\")\n",
    "print(f\"Intercept (b): {b_numpy}\")\n",
    "print(f\"Equation: y = {m_numpy:.2f}x + {b_numpy:.2f}\\n\")\n",
    "\n",
    "# TODO: Calculate predictions and errors\n",
    "y_pred_numpy = X_numpy @ solution\n",
    "mae_numpy = mean_absolute_error(y_numpy, y_pred_numpy)\n",
    "mse_numpy = mean_squared_error(y_numpy, y_pred_numpy)\n",
    "\n",
    "print(f\"MAE: {mae_numpy:.2f}\")\n",
    "print(f\"MSE: {mse_numpy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TASK 3: Scikit-learn LinearRegression (Full Dataset)\n",
    "\n",
    "\n",
    "\n",
    " Use sklearn's `LinearRegression` class.\n",
    "\n",
    "\n",
    "\n",
    " **Note:** sklearn automatically handles the intercept internally (with `fit_intercept=True` by default), so we don't need to add a column of ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reshape X for sklearn (needs 2D array)\n",
    "X_sklearn = df['Hours Studied'].values.reshape(-1,1)\n",
    "y_sklearn = df['Exam_Score'].values\n",
    "\n",
    "# TODO: Create and fit the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_sklearn,y_sklearn)\n",
    "\n",
    "# TODO: Extract coefficients\n",
    "m_sklearn = model.coef_[0]\n",
    "b_sklearn = model.intercept_\n",
    "\n",
    "print(\"TASK 3 - Sklearn LinearRegression (all 10 points):\")\n",
    "print(f\"Slope (m): {m_sklearn}\")\n",
    "print(f\"Intercept (b): {b_sklearn}\")\n",
    "print(f\"Equation: y = {m_sklearn:.2f}x + {b_sklearn:.2f}\\n\")\n",
    "\n",
    "# TODO: Calculate predictions and errors\n",
    "y_pred_sklearn = model.predict(X_sklearn)\n",
    "mae_sklearn = mean_absolute_error(y_sklearn, y_pred_sklearn)\n",
    "mse_sklearn = mean_squared_error(y_sklearn, y_pred_sklearn)\n",
    "\n",
    "print(f\"MAE: {mae_sklearn:.2f}\")\n",
    "print(f\"MSE: {mse_sklearn:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TASK 4: Comparison and Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARISON OF ALL THREE METHODS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<30} {'Slope':<12} {'Intercept':<12} {'MAE':<10} {'MSE':<10}\")\n",
    "print(\"-\"*70)\n",
    "# TODO: Fill in the comparison table with your results\n",
    "print(f\"{'Manual (5 points)':<30} {m_manual:<12.4f} {b_manual:<12.4f} {mae_manual:<10.2f} {mse_manual:<10.2f}\")\n",
    "print(f\"{'NumPy (10 points)':<30} {m_numpy:<12.4f} {b_numpy:<12.4f} {mae_numpy:<10.2f} {mse_numpy:<10.2f}\")\n",
    "print(f\"{'Sklearn (10 points)':<30} {m_sklearn:<12.4f} {b_sklearn:<12.4f} {mae_sklearn:<10.2f} {mse_sklearn:<10.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot all three regression lines\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['Hours_Studied'], df['Exam_Score'], color='black', s=100, label='Data', zorder=3)\n",
    "\n",
    "# TODO: Create x range for plotting\n",
    "x_range = np.linspace(df['Hours Studied'].min(), df['Hours Studied'].max(), 100)\n",
    "\n",
    "# TODO: Plot each regression line with different styles\n",
    "plt.plot(x_range, m_manual*x_range+b_manual '--', label=\"Manual (5 pts)\", linewidth=2)\n",
    "plt.plot(x_range, m_numpy*x_range+b_numpy '-', label=\"NumPy lstsq (10 pts)\", linewidth=2)\n",
    "plt.plot(x_range, m_sklearn*x_range+b_sklearn ':', label=\"Sklearn (10 pts)\", linewidth=3)\n",
    "\n",
    "plt.xlabel('Hours Studied')\n",
    "plt.ylabel('Exam Score')\n",
    "plt.title('Comparison of Regression Methods')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Discussion Questions\n",
    "\n",
    "\n",
    "\n",
    " **1. Why do the manual calculations differ from NumPy and sklearn?**\n",
    "\n",
    "\n",
    "\n",
    " Manual calculations only uses first five points so it's fitted to smaller dataset. NumPy and Sklearn use all 10 points giving a more accurate estimate of the true relationship.\n",
    "\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    "\n",
    "\n",
    " **2. Why are NumPy and sklearn results nearly identical?**\n",
    "\n",
    "\n",
    "\n",
    " They both solve the same problem.\n",
    "\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    "\n",
    "\n",
    " **3. Which method produced the lowest error? Why?**\n",
    "\n",
    "\n",
    "\n",
    " NumPy and sklearn because they used all 10 data points giving them a better fit of the data. Manual was only trained on the first five points so it couldn't capture the full trend.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## PART 2: MULTIPLE LINEAR REGRESSION\n",
    "\n",
    "\n",
    "\n",
    " Now we'll predict house prices using multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: House Prices\n",
    "house_data = {\n",
    "    'Size_sqft': [1200, 1500, 1800, 2000, 2200, 2500, 2800, 3000, 3200, 3500,\n",
    "                  1300, 1600, 1900, 2100, 2400, 2600, 2900, 3100, 3300, 3600],\n",
    "    'Bedrooms': [2, 3, 3, 3, 4, 4, 4, 4, 5, 5,\n",
    "                 2, 3, 3, 4, 4, 4, 4, 5, 5, 5],\n",
    "    'Age_years': [15, 20, 10, 5, 8, 3, 12, 6, 15, 2,\n",
    "                  18, 12, 8, 10, 5, 7, 4, 9, 11, 1],\n",
    "    'Price_1000s': [180, 210, 250, 280, 310, 350, 360, 400, 420, 480,\n",
    "                    190, 230, 270, 300, 340, 370, 390, 430, 450, 510]\n",
    "}\n",
    "house_df = pd.DataFrame(house_data)\n",
    "\n",
    "print(\"House Price Dataset:\")\n",
    "print(house_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TASK 5: Justify Multiple Linear Regression\n",
    "\n",
    "\n",
    "\n",
    " Calculate correlations to understand relationships between features and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlations with Price:\")\n",
    "# TODO: Calculate correlation between each feature and price\n",
    "for col in ['Size_sqft', 'Bedrooms', 'Age_years']:\n",
    "    corr = house_df[col].corr(house_df['Price_1000s'])\n",
    "    print(f\"{col}: {corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Why should we use multiple linear regression instead of simple linear regression?**\n",
    "\n",
    "\n",
    "\n",
    " Because there are multiple factors. Multiple linear regression will give us a more accurate prediction because it can take in multiple factors: size, # of bedrooms, age on house price.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TASK 6: Implement Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prepare features and target\n",
    "X_multi = house_df[['Size_sqft', 'Bedrooms', 'Age_years']]\n",
    "y_multi = house_df['Price_1000s']\n",
    "\n",
    "# TODO: Split into training and testing sets (80-20 split)\n",
    "# The purpose of splitting the data into a training and testing set is to save some of the data to test the model that we trained.\n",
    "# The test data set needs to be separate from the training data so we can simulate \"new\" situations before we put our model out in the real world.\n",
    "# Hint: There is a very helpful sklearn method you should use for this step\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_multi,y_multi,test_size=0.2)\n",
    "\n",
    "# TODO: Create and train the model\n",
    "mlr_model = LinearRegression()\n",
    "# TODO: Fit the model\n",
    "mlr_model.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Make predictions\n",
    "y_train_pred = mlr_model.predict(X_train)\n",
    "y_test_pred = mlr_model.predict(X_test)\n",
    "\n",
    "# TODO: Extract coefficients\n",
    "coefficients = mlr_model.coef_\n",
    "intercept = mlr_model.intercept_\n",
    "\n",
    "print(\"Model Coefficients:\")\n",
    "print(f\"Intercept: {intercept:.2f}\")\n",
    "for i, col in enumerate(X_multi.columns):\n",
    "    print(f\"{col}: {coefficients[i]:.2f}\")  # TODO: Fill in actual coefficient\n",
    "\n",
    "print(\"\\nModel Equation:\")\n",
    "print(\"Price = {intercept:.2f} + {coefficients[0]:.2f}*Size + {coefficients[1]:.2f}*Bedrooms + {coefficients[2]:.2f}*Age\")  # TODO: Complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TASK 7: Calculate and Interpret Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate training errors (how far were predictions from the real prices)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_rmse = np.sqrt(train_mse)\n",
    "\n",
    "# TODO: Calculate testing errors (how far were predictions from the real prices)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "\n",
    "print(\"Training Set Performance:\")\n",
    "print(f\"MAE: $(train_mae:.2f)k\")  \n",
    "print(f\"MSE: $(train_mse:.2f)k²\")  \n",
    "print(f\"RMSE: $(train_rmse:.2f)k\")  \n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "print(f\"MAE: $(test_mae:.2f)k\")  \n",
    "print(f\"MSE: $(test_mse:.2f)k²\")  \n",
    "print(f\"RMSE: $(test_rmse:.2f)k\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### TASK 8: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.6, s=100, color='blue')\n",
    "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0].set_xlabel('Actual Price ($1000s)')\n",
    "axes[0].set_ylabel('Predicted Price ($1000s)')\n",
    "axes[0].set_title('Training Set: Actual vs Predicted')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Testing set\n",
    "# TODO: Complete the test set plot\n",
    "axes[1].scatter(y_test, y_test_pred, alpha=0.6, s=100, color='green')\n",
    "axes[1].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[1].set_xlabel('Actual Price ($1000s)')\n",
    "axes[1].set_ylabel('Predicted Price ($1000s)')\n",
    "axes[1].set_title('Test Set: Actual vs Predicted')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Final Discussion Questions\n",
    "\n",
    "\n",
    "\n",
    " **1. How do the training and test errors compare? What does this tell you? (Recall our discussion on overfitting and underfitting from last week)**\n",
    "\n",
    "\n",
    " Good - training and test errors are similar and low;\n",
    " Overfitting - very low training error and high test error;\n",
    " Underfitting - high training and test errors;\n",
    "\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    "\n",
    "\n",
    " **2. Which feature has the strongest effect on house price? How can you tell?**\n",
    "\n",
    "\n",
    "\n",
    " Largest absolute coefficient because larger houses sell for more. \n",
    "\n",
    "\n",
    "\n",
    " ---\n",
    "\n",
    "\n",
    "\n",
    " **3. What is one limitation of this model?**\n",
    "\n",
    "\n",
    "\n",
    " It assumes a linear relationship between features and target. If the relationship is nonlinear it will either over or under estimate.\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
